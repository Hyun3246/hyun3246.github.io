---
title:  "[Hands-On ML] 19. Training and Deploying TensorFlow Models at Scale - 4"
excerpt: "Parallel Execution on Multiple Devices, Training Model on Multiple Devices"

categories:
  - Data Science & ML
tags:
  - [Machine Learning, Python]

use_math: true
toc: true
toc_sticky: true

date: 2025-02-06
last_modified_at: 2025-02-06

header:
  overlay_image: https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/overlay image/Hands-on ML.png
---
## Allocating Computations and Variables to Device
There are several points to check before allocating computations and variables.

- Generally, allocate preprocessing to CPU and NN computations to GPU.
- GPU has communication bandwidth. Therefore, avoid unnecessary data transfer using GPU.
- Adding RAM to CPU is easy, but to GPU is hard. Therefore, if a certain variable is unnecessary for the next few steps, allocate to CPU.

Basically, except in case that there is no GPU kernel, all variables and computations are allocated to GPU. Variables and computations without GPU kernel(e.g. integer variable or integer tensor) are allocated to CPU.

<br/>

## Parallel Execution on Multiple Devices
One of the strengths using TF function is parallelization. See the following example.
<br/>
<figure style="display:block; text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/Hands-On ML/parallelization of tf function.png"
       style="width: 40%; height: auto; margin:10px">
</figure>
<br/>

Basic steps are as follows.

1. Analyze graph and find the list of operations to evaluate.
2. Count how dependent each operation is on the other operations. (e.g. Dependency counter of F is 4.)
3. TF sends source operation with 0 dependence to evaluation queue of allocated device. (e.g. A is sent to CPU.)
4. If one operation is evaluated, dependency counter of all the other operations that depend on the operation decrease. (e.g. If A evaluated, dependency counter of F decreases from 4 to 3.)
5. Repeat step 3 and 4 until all operations are calculated.

Under the basic scenarios above, operations sent to CPU and GPU handled slightly different. 

- CPU
    - Sent to inter-op thread pool.
    - If operation has multi-thread CPU kernel, the operations is divided into multiple parts and allocated to different evaluation queue.

- GPU
    - Operations in GPU are evaluated in order.
    - Operations with multi-thread kernel(e.g. implemented with CUDA and cuDNN) use as many GPU threads as possible.

<br/>

## Training Model on Multiple Devices: Model Parallelism
First method to train model on multiple devices is to divide model into multiple devices. It is called <span style="color:#F5F5F7">model parallelism</span>.

Model parallelism is not always good.

- Fully connected NN <br/>
    <br/>
    <figure style="display:block; text-align:center;">
    <img src="https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/Hands-On ML/Model parallelism of fully connected NN.png"
        style="width: 40%; height: auto; margin:10px">
    </figure>
    <br/>

    There is no advantage of dividing model on fully connectd NN. When each layer is allocated to one device, one layer should wait for the prior layer. If you vertically divide the model, communications between two devices are needed, which can slow down the process.

- Conv NN <br/>
    <br/>
    <figure style="display:block; text-align:center;">
    <img src="https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/Hands-On ML/Model parallelism of CNN.png"
        style="width: 40%; height: auto; margin:10px">
    </figure>
    <br/>
    Partially connected NN such as Conv NN has advantage of vertical division.

- RNN <br/>
    <br/>
    <figure style="display:block; text-align:center;">
    <img src="https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/Hands-On ML/Model parallelism of RNN.png"
        style="width: 40%; height: auto; margin:10px">
    </figure>
    <br/>
    RNN is efficiently divided into multiple devices. See the image above in time scale. At first step, only one device is used. At the second step, two devices are used, and so on. Communications are still needed, but the flaw is smaller than the advantage of parallel computations. (Actually, stacking LSTM on one GPU is faster.)

<br/>

## Training Model on Multiple Devices: Data Parallelism
Seperating data to multiple devices are more general and efficient. First, copy model to multiple devices. After then, divide data into multiple mini batch and perform training step in the the same time. This is called <span style="color:#F5F5F7">data parallelism or SPMD</span>(Single Program, Multiple Data).

There are many variations, and one of the most important ones is <span style="color:#F5F5F7">mirrored strategy</span>.

In mirrored strategy, copy model parameters exactly equal to all GPUs and always apply same parameter update to all GPUs. Copied models are preserved in exactly same state.
<br/>
<figure style="display:block; text-align:center;">
<img src="https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/Hands-On ML/mirrored strategy.png"
    style="width: 40%; height: auto; margin:10px">
</figure>
<br/>

The most tricky part is to efficiently calculate gradient average from all GPUs and distribute the result to all of them. <span style="color:#F5F5F7">AllReduce</span> algorithms can handle this. The algorithm is that multiple nodes corporates to perform reduce operation(same operation of python `reduce`, which is useful to calculate average).

Another method is to save model parameter outside of GPU(worker). The parameters are stored in parameter server and updated and distributed only through the server.
<br/>
<figure style="display:block; text-align:center;">
<img src="https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/Hands-On ML/parameter server.png"
    style="width: 40%; height: auto; margin:10px">
</figure>
<br/>

To update the parameters in parameter server, there are two ways, synchronous update and asynchronous update.

Synchronous update
- Waits for all calculations and update parameters at once
- Pros
    - Free from stale gradient(see asynchronous update for meaning).
- Cons
    - Must wait for slow device for each step.
    - As parameters are distributed at the same time, bandwidth may become saturated.

Just looking at the above, asynchronous update seems better. However, there is a critical problem in asynchornous update.

Asynchronous update
- Frequently updates parameters when each model's calculation ends.
- Pros
    - Free from waiting other devices.
    - Free from bandwidth saturation.
- Cons
    - Stale(old) gradient. If one model already updates the parameters, gradients of the other ones could direct wrong direction in the new point. This could make the training unstable.
        <br/>
        <figure style="display:block; text-align:center;">
        <img src="https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/Hands-On ML/Stale gradient.png"
            style="width: 40%; height: auto; margin:10px">
        </figure>
        <br/>

There are a few ways to lessen stale gradient(e.g. low learning rate, use one device only for first several epochs), but synchronous update is more efficient than asynchronous one.

<br/>

[Go for Codes](https://github.com/Hyun3246/Warehouse/blob/1935957ad2bfdd8b9664e43371caa32e43e349d5/Hands-On%20ML/Chapter_19_Training_and_Deploying_TensorFlow_Models_at_Scale.ipynb)


<br/>
<br/>

*All images, except those with separate source indications, are excerpted from lecture materials.*