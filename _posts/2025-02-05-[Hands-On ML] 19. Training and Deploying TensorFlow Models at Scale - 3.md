---
title:  "[Hands-On ML] 19. Training and Deploying TensorFlow Models at Scale - 3"
excerpt: "Model on mobile, embeded and webpage. Using GPU to speed up."

categories:
  - Data Science & ML
tags:
  - [Machine Learning, Python]

use_math: true
toc: true
toc_sticky: true

date: 2025-02-05
last_modified_at: 2025-02-05

header:
  overlay_image: https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/overlay image/Hands-on ML.png
---
## Distributing Model on Mobile or Embeded Devices
Model or embeded devices are ususally smaller than powerful GPU servers. Therefore, despite of the strengths of edge computing(computing far from the central server) such as security and unnecessary internet connection, <span style="color:#F5F5F7">lighter and more efficient model is needed</span>.

TFLite provides multiple tools to achieve following goals.

- Diminish the size of model to decrease downloading time and RAM usage.
- Decrease the computation amount to lessen loading time, battery usage, and heat.
- Fit the model to specific condition of device.

Model transformer of TFLite receives SavedModel and compresses to lightweight format of FlatBuffers. FlatBuffers can be loaded to RAM without any preprocessing. Therefore, it decreases the loading time and memory usage.

Transformer also performs model optimization. For example, batch normalization could be merged to sum and multiplication of the prior layer.

Instead of using smaller(optimized) NN, another way to decreased the size of model is to using smaller bit length. For example, use half-float integer(16bit) instead of single precision(32bit). Accuracy of model will go down, but the size will decrease into half.

TFLite also compresses the weigths of the model into 8-bit integer. The simplest way is <span style="color:#F5F5F7">post-training quantization</span>. After training, finds the maximum absoulute value m of the weights and maps -m ~ m to -127 ~ 127. 0.0 always maps to 0. The following image is an example of -1.5 to 0.8 mapping.
<br/>
<figure style="display:block; text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/Hands-On ML/post-training quantization.png"
       style="width: 40%; height: auto; margin:10px">
</figure>
<br/>

Activation output can also be quantized. <span style="color:#F5F5F7">Whereas weight quantization only decrease the size of application, quantizing output of activation decreases response delay and power consumption</span>.

As mentioned, the problem of quantization is sacrificing accuracy. This is similar to adding noise to weights and activation value. If the loss of accuracy is too big, we can do <span style="color:#F5F5F7">quantization-aware training</span>. By adding fake quantization calculation to the model, we can train the model to ignore quantization noise.

<br/>

## Running Model on Webpage
We can use TFJS library to run model on webpage. We can also convert the website to progressive web app(PWA). TFJS also supports directly training model on web browser!

<br/>

## Using GPU to Speed Up
GPU can speed up calculation. TensorFlow controls GPU as follows.
<br/>
<figure style="display:block; text-align:center;">
  <img src="https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/Hands-On ML/tensorflow and gpus.png"
       style="width: 40%; height: auto; margin:10px">
</figure>
<br/>

<br/>

## Managing GPU RAM
Basically, tensorflow secures almost all RAM of GPUs to prevent RAM fragmentation. However, this could cause RAM shortage. There are a several ways to solve the problem.

1. Assigning each GPU to one process.
2. Making tensorflow occupy only certain size of RAM of GPU. (e.g. 2GiB per GPU)
3. Making tensorflow occupy GPU only if necessary.
4. Dividing GPU into 2+ logical devices. (e.g. Divide one GPU into 2 to test multi-GPU algorithm.)

For specific codes, see 'Go for Codes'.

<br/>

[Go for Codes](https://github.com/Hyun3246/Warehouse/blob/859b1a917836fd90b82175751f699b53de7135f3/Hands-On%20ML/Chapter_19_Training_and_Deploying_TensorFlow_Models_at_Scale.ipynb)


<br/>
<br/>

*All images, except those with separate source indications, are excerpted from lecture materials.*