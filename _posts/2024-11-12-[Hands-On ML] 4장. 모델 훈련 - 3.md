---
title:  "[Hands-On ML] 4장. 모델 훈련 - 3"
excerpt: "로지스틱 회귀와 softmax"

categories:
  - Data Science & ML
tags:
  - [머신러닝, 파이썬, 분류, 로지스틱 회귀, softmax]

use_math: true
toc: true
toc_sticky: true

date: 2024-11-12
last_modified_at: 2024-11-12

header:
  overlay_image: https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/overlay image/Hands-on ML.png
---
## 로지스틱 회귀

[Deep Learning Basic Starting with TF](https://hyun3246.github.io/data%20science%20&%20ml/Deep-Learning-Basic-Starting-with-TF-5-1.-Logistic-Regression-Classification%EC%9D%98-%EC%86%8C%EA%B0%9C/#%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80)

<br/>

## 로지스틱 회귀의 비용 함수

[Deep Learning Basic Starting with TF](https://hyun3246.github.io/data%20science%20&%20ml/Deep-Learning-Basic-Starting-with-TF-5-2.-Logistic-Regression-Classification%EC%9D%98-cost-%ED%95%A8%EC%88%98,-%EC%B5%9C%EC%86%8C%ED%99%94/)

모든 훈련 샘플의 비용을 평균하여 전체 train set에 대한 비용 함수를 구할 수 있다. 이를 로그 손실이라고 한다.

$$J(\theta) = -\frac{1}{m} \displaystyle \sum_{i=1}^{m}{[y^{(i)} \log{\hat{p}^{(i)}} + (1 - y^{(i)}) \log{(1 - \hat{p}^{(i)})}]}$$

안타깝게도, 위 함수의 최솟값은 알려진 해가 없다고 한다. 정규방정식처럼 쉽게 해를 구할 수가 없다는 뜻이다. 하지만 볼록 함수이므로 무조건 전역 최솟값을 찾을 수는 있다고 한다. j번째 파라미터 $\theta_j$ 에 대해 편미분을 하면 다음과 같다.

$$\frac{\partial}{\partial \theta_j} = \frac{1}{m} \displaystyle \sum_{i=1}^{m}{(\sigma(\theta^T x^{(i)}) - y^{(i)})x_j^{(i)}}$$

<br/>

## 로지스틱 회귀의 구현
붓꽃 분류 데이터를 활용하여 로지스틱 회귀를 구현하는 코드는 다음과 같다.

```python
# 데이터 분할하고 로지스틱 회귀 훈련
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X = iris.data[["petal width (cm)"]].values
y = iris.target_names[iris.target] == 'virginica'
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)
```

모델의 추정 확률을 계산하여 모델이 분류를 진행한 기준을 살펴볼 수 있다. 시각화도 가능하다.

```python
# 추정 확률 계산
import numpy as np
import matplotlib.pyplot as plt

X_new = np.linspace(0, 3, 1000).reshape(-1, 1)
y_proba = log_reg.predict_proba(X_new)
decision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]       # 모델이 분류한 기준선
```

위 예시에서는 꽃잎 너비가 `decision_boundary`보다 크면 붓꽃으로 분류한다.

물론 2개 이상의 특성을 가지고 분류할 수도 있다.

```python
X = iris.data[["petal length (cm)", "petal width (cm)"]].values
y = iris.target_names[iris.target] == 'virginica'
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

log_reg = LogisticRegression(C=2, random_state=42)
log_reg.fit(X_train, y_train)
```

<br/>

## Softmax 회귀
로지스틱 회귀는 이진 분류기를 여러 개 연결하지 않고도 직접 다중 클래스를 지원할 수 있다. 이를 softmax 회귀라고 한다. 과정은 간단하다.

1. 샘플 x에 대해, softmax 회귀 모델이 각 클래스 k에 대한 점수인 $s_k(x)$ 계산.
2. 위 점수에 softmax 함수 적용하여 클래스의 확률 추정.

Softmax 함수는 다음과 같다.

$$\hat{p}_k = \frac{e^{s_k(x)}}{\displaystyle \sum_{j=1}^{K}{e^{s_j(x)}}}$$

K는 클래스의 수이다.

추정 확률이 가장 높은 클래스를 `argmax`로 선택하기만 하면 된다.

> cf. Softmax 회귀 분류는 한 번에 하나의 클래스만 예측할 수 있다. 따라서 여러 사람의 얼굴을 인식하는 것과 같은 분류를 하려면 다른 방법을 써야 한다.

Softmax의 비용 함수는 cross-entropy를 사용한다.

[Deep Learning Basic Starting with TF](https://hyun3246.github.io/data%20science%20&%20ml/Deep-Learning-Basic-Starting-with-TF-6-2.-Softmax-Classifier%EC%9D%98-cost%ED%95%A8%EC%88%98/#softmax%EC%9D%98-%EB%B9%84%EC%9A%A9%ED%95%A8%EC%88%98)

코드 구현은 다음과 같다.

```python
# softmax 회귀 사용
X = iris.data[["petal length (cm)", "petal width (cm)"]].values
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

softmax_reg = LogisticRegression(C=30, random_state=42)
softmax_reg.fit(X_train, y_train)
softmax_reg.predict([[5, 2]])
softmax_reg.predict_proba([[5, 2]]).round(2)        # 추정 확률 반환
```

[전체 코드 보러가기](https://github.com/Hyun3246/Code-Warehouse/blob/248134d8d3e948d5dbf635b3677a3656485475eb/Hands-On%20ML/Chapter_04_02_Logistic_Regression.ipynb)

<br/>
<br/>

*별도의 출처 표시가 있는 이미지를 제외한 모든 이미지는 강의자료에서 발췌하였음을 밝힙니다.*