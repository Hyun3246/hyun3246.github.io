---
title:  "[Hands-On ML] 9장. 비지도 학습 - 3"
excerpt: "Gaussian Mixture Model"

categories:
  - Data Science & ML
tags:
  - [머신러닝, 파이썬, 비지도 학습]

use_math: true
toc: true
toc_sticky: true

date: 2024-11-29
last_modified_at: 2024-11-29

header:
  overlay_image: https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/overlay image/Hands-on ML.png
---
## 가우스 혼합
<span style="color:#F5F5F7">가우스 혼합 모델(Gaussian Mixture Model, GMM)은 샘플이 파라미터가 알려지지 않은 여러 개의 혼합된 가우스 모델에서 생성되었다고 가정하는 확률 모델</span>이다. 하나의 가우스 분포에서 생성된 모든 샘플은 하나의 cluster를 형성하며, 일반적으로 타원형이다.

가장 간단하게 GMM을 구현하는 것은 `GaussinaMixture` 클래스를 이용하면 된다. 사전에 cluster 개수 k를 지정해야 한다.

```python
from sklearn.mixture import GaussianMixture

gm = GaussianMixture(n_components=3, n_init=10, random_state=42)
gm.fit(X)
```
위 클래스는 기댓값-최대화(Expectation-Maximization) 알고리즘을 사용하여 다음과 같이 작동한다.

1. 기댓값 단계: 샘플을 클러스터에 할당한다(각 클러스터에 속할 확률을 예측한다).
2. 최대화 단계: 클러스터를 업데이트 한다(각 클러스터가 dataset에 있는 모든 샘플을 사용해 업데이트 된다).

k-평균과 비슷해보이지만 소프트 클러스터 할당을 사용한다는 점이 다르다. 클러스터에 속할 확률로 샘플의 가중치가 적용되는데, 이 확률을 책임(responsibility)이라고 한다. 최대화 단계에서 클러스터 업데이트는 책임이 가장 많은 샘플의 영향을 크게 받는다.

하드 클러스터는 `predict()`, 소프트 클러스터는 `predict_proba()`를 사용한다.

GMM은 생성 모델로, 새로운 샘플을 생성할 수도 있다.

```python
X_new, y_new = gm.sample(6)
```

GMM은 거의 항상 타원형의 cluster를 만드려고 하기 때문에 항상 잘 작동하는 것은 아니다. 예를 들어, 초승달과 같은 분포를 가진 dataset에서는 제대로 된 cluster를 구하기 어렵다.

<br/>

## 가우스 혼합 규제
특성이나 cluster가 많거나, 샘플이 적으면 EM이 최적의 솔루션으로 수렴하기 어렵다. Cluster의 모양과 방향의 범위를 제한하여 문제를 해결할 수 있다. `covariance_type`을 `spherical`, `diag`, `tied` 등으로 설정하여 cluster의 모양을 제한할 수 있다. 기본값은 `full`로 제약이 없다.

<br/>

## 가우스 혼합 이상치 탐지
GMM으로 이상치를 탐지하는 것은 간단하다. 밀도가 낮은 지역에 있는 샘플을 이상치로 볼 수 있으며, 우리가 해야할 것은 임곗값을 지정해주는 것 뿐이다.

```python
densities = gm.score_samples(X)
density_threshold = np.percentile(densities, 2)
anomalies = X[densities < density_threshold]
```
> cf. 특이치 탐지 <br/> 특이치 탐지는 이상치 탐지와 비슷하지만, 아예 이상치가 없는 깨끗한 dataset으로 훈련한다는 점이 다르다.

<br/>

## 가우스 혼합 Cluster 개수 선택
GMM에서는 inertia나 silhouette score를 사용할 수 없다. 타원형 cluster에는 부적절하기 때문이다. 대신 <span style="color:#F5F5F7">BIC(Bayesian Information Criterion)나 AIC(Akaike Information Criterion)</span>를 이용할 수 있다.

$$BIC = \log{(m)}p - 2 \log(\hat{\mathcal{L}})$$

$$AIC = 2p - 2 \log(\hat{\mathcal{L}})$$

m은 샘플의 개수, p는 모델이 학습할 파라미터의 개수, $\mathcal{L}$ 은 모델의 가능도 함수(likelihood function)의 최댓값이다.

BIC와 AIC는 모두 학습할 파라미터가 많은(클러스터가 많은) 모델에 벌칙을 가하고, 데이터에 잘 맞는 모델에 보상을 준다. BIC가 선택한 모델이 AIC가 선택한 모델보다 간단한 경향이 있다.

```python
gm.bic(X)
gm.aic(X)
```

Cluster 개수에 따른 BIC와 AIC의 변동 추세를 그래프로 나타내면 최적의 파라미터 개수(BIC와 AIC 모두 낮은 지점)를 쉽게 정할 수 있다.

`BayesianGaussianMixture` 클래스를 사용하면 cluster 개수를 수동으로 정하는 대신 자동으로 불필요한 cluster를 제거하도록 할 수 있다. `n_components`를 적당히 큰 값(예측된 최적 cluster 개수보다 큰 값)으로 지정해주기만 하면 된다.

```python
from sklearn.mixture import BayesianGaussianMixture
bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)
bgm.fit(X)
bgm.weights_.round(2)       # 최적 cluster보다 큰 cluster의 weights는 모두 0으로 출력됨.
```

<br/>

## 기타 이상치, 특이치 탐지 알고리즘

1. Fast-MCD
2. Isolation Forest
3. LOF
4. one-class SVM
5. PCA 또는 다른 차원 축소 알고리즘


[코드 보러가기](https://github.com/Hyun3246/Code-Warehouse/blob/63c2f964f1f87324534a135d550d91c3d9df0425/Hands-On%20ML/Chapter_09_Unsupervised_Learning.ipynb)


<br/>
<br/>

*별도의 출처 표시가 있는 이미지를 제외한 모든 이미지는 강의자료에서 발췌하였음을 밝힙니다.*
