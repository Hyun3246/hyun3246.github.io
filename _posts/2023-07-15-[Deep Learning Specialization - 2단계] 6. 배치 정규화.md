---
title:  "[Deep Learning Specialization - 2단계] 6. 배치 정규화"
excerpt: "배치 정규화"

categories:
  - Data Science & ML
tags:
  - [머신러닝, 정규화, 미니 배치, 배치 정규화]

use_math: true
toc: true
toc_sticky: true
 
date: 2023-07-15
last_modified_at: 2023-08-05

header:
  overlay_image: https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/overlay image/andrew ng 2.png
---
## 배치 정규화
정규화에 대해서는 이미 앞에서 살펴본 적이 있다. 정규화를 하면 입력 값들의 평균과 표준편차를 조정할 수 있고, 경사하강법 적용을 더 효과적으로 할 수 있다고 했다.

[정규화 내용 복습하기](https://hyun3246.github.io/machine%20learning/Deep-Learning-Specialization-2%EB%8B%A8%EA%B3%84-3.-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%AC%B8%EC%A0%9C-%EC%84%A4%EC%A0%95/)

심층 신경망에서도 정규화를 할 수 있다. 이 경우, 각 신경유닛들에서 계산된 값인 $a^{[l]}$들을 정규화에 이용하면 더 효과적일 것이다. 많은 논쟁이 있긴 하지만, 사실 실제로는 활성함수에 적용하기 이전의 값인 $z^{[l]}$을 더 많이 사용한다.

배치 정규화는 다음과 같은 과정으로 진행된다.

$\displaystyle \mu = \frac{1}{m}\sum_{i}{z^i}$

$\displaystyle \sigma^2 = \frac{1}{m}\sum_{i}{(z_i-\mu)^2}$

$z_{norm}^{(i)} = \frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}$

$\tilde{z}^{(i)} = \gamma z_{norm}^{(i)} + \beta$

$\tilde{z}^{(i)}$ 을 $z^{(i)}$ 대신 사용한다.

위 과정의 세 번째 줄에서 $\epsilon$을 분모에 더하는 것은 분모가 0이 될 경우를 방지하기 위해 수학적 안정성을 더해주는 것이다. 네 번째 줄에서 등장하는 $\gamma$, $beta$는 학습 매개변수로, RMSprop등의 알고리즘을 통한 학습으로 얻어질 수 있다. 네 번째 과정을 통해 선형 변환을 해주는 이유는 <font color='#F5F5F7'>은닉유닛이 항상 동일한 분포를 가지지 않도록 조절</font>해주기 위함이다.

만약 $\gamma = \sqrt{\sigma^2+\epsilon}$, $\beta = \mu$라면 $\tilde{z}^{(i)} = z^{(i)}$일 것이다.

배치 정규화의 의의는 단지 입력층뿐만 아니라 깊은 신경망 안에서까지 정규화를 해준다는 것에 있다. 차이점이라면 배치 정규화에서는 평균과 표준편차가 0과 1에 고정되지 않도록 선형 변환을 해준다는 것이다.

<br/>

## 배치 정규화 적용하기
배치 정규화를 신경망에 적용하는 과정은 단순하다. w와 b를 이용해서 구해진 z값에 정규화를 적용해주면 되는 것이다. 그 다음 그 값을 활성함수에 넣어 최종적인 a값을 구한다.

그리고 train set의 미니 배치에서도 적용할 수 있다. 각각의 미니 배치에서 정규화를 할 때는 그 미니 배치만 이용해서 평균과 분산을 구한다.

각 층에서 사용되는 매개변수로는 $W^{[l]}$, $b^{[l]}$, $\beta^{[l]}$, $\gamma^{[l]}$ 등이 있다. 중요한 것은, b는 제외하고 생각해도 된다는 것이다. 어차피 <font color='#F5F5F7'>정규화 과정에서 평균을 구할 때 상수인 b는 제거</font>되기 때문이다. b의 역할을 $\beta$가 대신한다고 생각하면 된다.

이제 경사하강법에 적용해보자.

```
for t = 1 ... num_Minibatches:
  Compute forward prop on X^{t}
    In each hidden layer, use BN to replace z^[l] with tilde_z^[l]
  
  Use backprop to compute dW^[l], d_beta^[l], d_gamma^[l]

  Update parameters W, beta, gamma with learning rate
```

위 과정은 RMSprop, Adam을 사용하는 경사하강법에도 적용할 수 있다.

<br/>

## 배치 정규화가 잘 작동하는 이유
공변량 변화라는 개념이 있다. 이는 X에서 Y로 대응하는 학습을 하는 과정에서, X의 분포가 변화하면 모델을 다시 학습해야한다는 것이다. 바뀌기 전 후의 관측함수가 변화하지 않더라도 말이다. 예를 들어, 고양이 판별 프로그램을 만든다고 가정하자. 학습에서는 검은색 고양이 사진만을 이용하고, 실제 판별에서는 색깔이 있는 고양이를 이용하면 결과가 좋지 못할 것이다. 비록 검은색 고양이 사진으로 만든 관측함수가 색깔이 있는 고양이로 만든 관측함수와 동일할지라도 다시 학습을 하기 전까지는 모르는 일이다.

공변량 변화는 배치 정규화를 해야 하는 이유와 관련이 있다. 신경망에서 배치 정규화를 적용하지 않으면 이전 신경층의 매개변수 변화에 많은 영향을 받을 것이다. 정규화를 통해 얻을 수 있는 효과인 '평균과 분산의 고정 효과'를 기대할 수 없기 때문이다. <font color='#F5F5F7'>즉, 배치 정규화를 하면 앞선 신경층의 결과 분포를 제한할 수 있기 때문에 공변량 변화의 문제에서 비교적 자유롭게 된다.</font> 앞 층과 뒷 층의 매개변수의 상관 관계를 줄여주어 각 층의 독립적인 학습이 가능해져 학습 속도를 향상시키는 효과도 있다.

미니 배치 정규화에서는 각각의 미니 배치에 대해 평균과 분산을 구하기 때문에 전체 데이터에 비해 노이즈가 있다. 이 때문에 $z^{[l]}$를 $\tilde{z}^{[l]}$로 변환하는 과정에도 노이즈가 끼어 있을 것이다. 즉, dropout처럼 각 신경층에 노이즈가 발생한다. 0과 1의 곱셈에 의한 노이즈만 있었던 dropout에 비해, 배치 정규화에서는 분산으로 나누면서 생기는 곱셈 노이즈, 평균을 빼면서 생기는 덧셈 노이즈가 모두 존재한다. <font color='#F5F5F7'>은닉층에 노이즈를 추가하는 것은 이후 하나의 은닉층에 너무 의존하지 않도록 하는 일반화 효과가 있다.</font> 큰 미니 배치를 이용할 수록 노이즈가 줄어들어 일반화 효과가 떨어질 것이다. 그러나 배치 정규화는 일반화를 목적으로 만들어진 것이 아니기 때문에 이 효과를 기대하고 사용하는 것은 추천하지 않는다.

<br/>

## 테스트에서의 배치 정규화
테스트 시에는 배치가 하나이기 때문에 평균과 분산을 계산할 수 없다. 따라서, 학습 시에 사용된 미니 배치들의 지수 가중 이동 평균을 추정치로 사용한다.
