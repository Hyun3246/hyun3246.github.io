---
title: "[Articles] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation"
excerpt: "An overview of StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model."
categories:
  - Data Science & ML
tags:
  - [Machine Learning, Computer Vision, GAN, Image-to-Image Translation, StarGAN]
use_math: true
toc: true
toc_sticky: true
date: 2025-09-04
last_modified_at: 2025-09-04
header:
  overlay_image: https://cdn.jsdelivr.net/gh/Hyun3246/hyun3246.github.io@master/image/overlay%20image/Research%20Paper.png
---

## About this Article
- **Authors**: Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
- **Year**: 2018
- **Official Citation**: Choi, Y., Choi, M., Kim, M., Ha, J. W., Kim, S., & Choo, J. (2018). StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8789-8797).

<br/>

## Accomplishments
- Proposed StarGAN, which is good at multi-domain image transformation by using only a single model.

<br/>

## Key Points

### 1. Architecture and Terms
- **attribute**: a meaningful feature inherent in an image (e.g. hair color, gender, age).
- **attribute value**: a particular value of an attribute (e.g. black hair, male).
- **domain**: a set of images sharing the same attribute value.

To train mapping among k domains, previous GANs need $k(k-1)$ generators. StarGAN solved this problem by using both the image and target domain information as inputs, training the model to flexibly translate the input into the corresponding domain with a single generator.

Like a standard GAN, it has a Generator (G) and a Discriminator (D). G generates an image, and D distinguishes if it is real or fake. In StarGAN, D also produces a probability distribution over domain labels, denoted as $D:x \rightarrow \{D_{src}(x), D_{cls}(x)\}$.

### 2. Training Objectives and Process

#### 1. Adversarial Loss
This loss is similar to the original GAN objective, but includes the target domain label c. G tries to minimize the objective, while D tries to maximize it.

$$ L\_{adv} = \mathbb{E}\_ {x} \big [ \log D\_{\mathrm{src}}(x) \big] + \mathbb{E}\_{x,c}\big[ \log \big( 1 - D\_{\mathrm{src}}(G(x,c)) \big) \big] $$


- $D_{src}(x)$: The probability that D classifies a real image x as real.
- $G(x,c)$: An image generated by G with target domain c.
- The generator G wants to make $D_{src}(G(x,c))$ go to 1 (fooling the discriminator), which makes the second term go to -âˆž, thus minimizing the overall objective.
- The formula is often replaced with the Wasserstein GAN objective with gradient penalty for stable training:
  
$$ L\_{adv} = \mathbb{E}\_{x} \big[ D\_{src}(x) \big] - \mathbb{E}\_{x,c} \big[ D\_{src}(G(x,c)) \big] - \lambda\_{gp}\mathbb{E}\_{\hat{x}}\big[(\lVert \nabla\_{\hat{x}}D\_{src}(\hat{x}) \rVert\_{2}-1)^{2}\big] $$

#### 2. Domain Classification Loss
An auxiliary objective is added to ensure generated images are classified as the target domain. It is split into two parts for D and G.

- **For Discriminator (D)**: D is trained to correctly classify a real image to its original domain $c'$. D minimizes this loss.
  
  $$ L\_{cls}^{r} = \mathbb{E}\_{x,c'} \big[ -\log D\_{cls}(c'|x) \big] $$
  
- **For Generator (G)**: G is trained to generate images that D will classify as the target domain c. G also minimizes this loss.
  
  $$ L\_{cls}^{f} = \mathbb{E}\_{x,c} \big[ -\log D\_{cls}(c | G(x,c)) \big] $$

#### 3. Reconstruction Loss
To ensure the generator only changes the target domain attributes while preserving other content, a cycle consistency loss is added.

$$ L\_{rec} = \mathbb{E}\_{x,c,c'} \big[ \lVert x - G(G(x,c),c') \rVert\_{1} \big] $$

- **Steps**:
    (1) Generate an image $G(x,c)$ by translating the input image x to the target domain c.
    (2) Use the generated image as input and translate it back to the original domain $c'$.
    (3) The final reconstructed image should be similar to the original input image x. The L1 norm of their difference is minimized.

#### 4. Full Objective
The final objectives for D and G combine the losses above, with hyperparameters $\lambda_{cls}$ and $\lambda_{rec}$ to control their relative importance.
- **For D**: $L_{D} = -L_{adv} + \lambda_{cls}L_{cls}^{r}$
- **For G**: $L_{G} = L_{adv} + \lambda_{cls}L_{cls}^{f} + \lambda_{rec}L_{rec}$

### 3. Training with Multiple Datasets

#### 1. Mask Vector
To handle multiple datasets with different sets of labels, a mask vector m is introduced. It allows the model to ignore unspecified labels and focus on the labels provided by a particular dataset. The unified label is represented as:

$$ \tilde{c}=[c\_{1}, \ldots, c\_{n}, m] $$

- $c_{i}$: A vector for the labels of the i-th dataset.
- m: A one-hot vector indicating which dataset the image comes from.

#### 2. Training Strategy
When training with an image from a specific dataset, the discriminator D tries to minimize only the classification error associated with that dataset's known labels. By alternating between datasets, D learns all discriminative features, and G learns to control all available labels.

<br/>

## Figures & Table Explanation

### 1. CelebA Dataset Experiments
- **Task**: Facial attribute transfer (hair color, gender, age).
- **Qualitative Results (Fig. 4)**: StarGAN showed higher visual quality compared to baselines. This success is attributed to the regularization effect of its multi-task learning framework and its use of convolutional activation maps as latent representations, which helps preserve facial identity.
- **Quantitative Results (Table 1, 2)**: In user studies, StarGAN received the majority of votes, especially in more complex multi-attribute transfer tasks.

### 2. RaFD Dataset Experiments
- **Task**: Facial expression synthesis (e.g., happy, sad).
- **Qualitative Results (Fig. 5)**: StarGAN generated the most natural-looking expressions while maintaining personal identity. This is due to an implicit data augmentation effect from using all available domains for training, which improves output quality and sharpness.
- **Quantitative Results (Table 3)**: StarGAN achieved the lowest classification error for its generated expressions and required significantly fewer parameters than cross-domain models like DIAT and CycleGAN.

### 3. CelebA + RaFD Joint Training Experiments
- **Task**: Using features learned from both datasets simultaneously.
- **Results (Fig. 6)**: The jointly trained model (StarGAN-JNT) produced higher quality images than the model trained only on a single dataset (StarGAN-SNG).

- **Mask Vector Role (Fig. 7)**: An experiment showed that providing the wrong mask vector (e.g., telling the model to use CelebA labels when an expression label was given) caused the model to ignore the expression and instead modify a CelebA attribute (age). This confirmed the mask vector works as intended, forcing the model to focus only on labels from the designated dataset.



